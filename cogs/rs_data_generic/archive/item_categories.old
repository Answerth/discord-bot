#!/usr/bin/env python3
import os
import json
import time
from math import ceil
from urllib.parse import quote, urlparse
from tqdm import tqdm
import logging
import requests
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

# ============================
# Configuration and Constants
# ============================

# Category Mapping (ID to Name)
CATEGORY_MAPPING = {
    0: "Miscellaneous",
    1: "Ammo",
    2: "Arrows",
    3: "Bolts",
    4: "Construction materials",
    5: "Construction products",
    6: "Cooking ingredients",
    7: "Costumes",
    8: "Crafting materials",
    9: "Familiars",
    10: "Farming produce",
    11: "Fletching materials",
    12: "Food and Drink",
    13: "Herblore materials",
    14: "Hunting equipment",
    15: "Hunting Produce",
    16: "Jewellery",
    17: "Mage armour",
    18: "Mage weapons",
    19: "Melee armour - low level",
    20: "Melee armour - mid level",
    21: "Melee armour - high level",
    22: "Melee weapons - low level",
    23: "Melee weapons - mid level",
    24: "Melee weapons - high level",
    25: "Mining and Smithing",
    26: "Potions",
    27: "Prayer armour",
    28: "Prayer materials",
    29: "Range armour",
    30: "Range weapons",
    31: "Runecrafting",
    32: "Runes, Spells and Teleports",
    33: "Seeds",
    34: "Summoning scrolls",
    35: "Tools and containers",
    36: "Woodcutting product",
    37: "Pocket items",
    38: "Stone spirits",
    39: "Salvage",
    40: "Firemaking products",
    41: "Archaeology materials",
    42: "Wood spirits",
    43: "Necromancy armour"
}

# Base URLs
BASE_CATEGORY_URL = "https://services.runescape.com/m=itemdb_rs/api/catalogue/category.json?category={}"
BASE_ITEMS_URL = "https://services.runescape.com/m=itemdb_rs/api/catalogue/items.json?category={}&alpha={}&page={}"

# Configuration
OUTPUT_DIR = "category_logs"
LOG_DIR = "logs"
REQUEST_DELAY = 0.5  # Base delay between requests in seconds

# Ensure output directories exist
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

# Logging Configuration
logging.basicConfig(
    level=logging.INFO,  # Set to INFO for general logs
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(os.path.join(LOG_DIR, "item_categories.log")),
        logging.StreamHandler()  # Logs also printed to console
    ]
)

# Database Configuration Path
DB_CONFIG_PATH = os.path.join(os.path.expanduser("~"), 'discordbot10s', 'dbconfig.json')

# ============================
# Helper Functions
# ============================

def load_db_config(config_path):
    """
    Load database configuration from a JSON file.
    The JSON file should have the following structure:
    {
        "username": "your_db_username",
        "password": "your_db_password",
        "host": "your_db_host",
        "port": "your_db_port",
        "database": "your_db_name"
    }
    """
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
        logging.info("Database configuration loaded successfully.")
        return config
    except Exception as e:
        logging.error(f"Failed to load database configuration: {e}")
        raise

def create_db_engine(config):
    """Create a SQLAlchemy engine."""
    try:
        engine = create_engine(
            f"postgresql://{config['username']}:{config['password']}@"
            f"{config['host']}:{config['port']}/{config['database']}"
        )
        logging.info("Database engine created successfully.")
        return engine
    except Exception as e:
        logging.error(f"Failed to create database engine: {e}")
        raise

def is_valid_url(url):
    """
    Validates the given URL.
    Returns True if the URL has a valid scheme and netloc, False otherwise.
    """
    parsed = urlparse(url)
    return all([parsed.scheme, parsed.netloc, parsed.path])

def create_session():
    """
    Creates a requests Session with retry strategy and custom headers.
    """
    session = requests.Session()
    retry = requests.packages.urllib3.util.retry.Retry(
        total=5,  # Total number of retries
        backoff_factor=1,  # Exponential backoff factor (1, 2, 4, 8, 16 seconds)
        status_forcelist=[429, 500, 502, 503, 504],  # Retry on these HTTP status codes
        allowed_methods=["GET"]  # Retry only on GET requests
    )
    adapter = requests.adapters.HTTPAdapter(max_retries=retry)
    session.mount("https://", adapter)
    session.mount("http://", adapter)

    # Set custom headers
    session.headers.update({
        "User-Agent": "RuneScapeItemCategorizer/1.0 (https://yourdomain.com/contact)",
        "Accept": "application/json"
    })

    return session

def fetch_json(url, session):
    """
    Fetch JSON data from a URL with retry handling.
    """
    if not is_valid_url(url):
        logging.error(f"Malformed URL detected: {url}")
        return None

    try:
        response = session.get(url, timeout=10)
        response.raise_for_status()
        if response.content:
            data = response.json()
            logging.info(f"Successfully fetched data from {url}")
            return data
        else:
            logging.warning(f"Empty response from {url}")
            return None
    except requests.exceptions.HTTPError as http_err:
        logging.error(f"HTTP error occurred for URL {url}: {http_err}")
        return None
    except requests.exceptions.RequestException as req_err:
        logging.error(f"Request exception for URL {url}: {req_err}")
        return None
    except json.JSONDecodeError as json_err:
        logging.error(f"JSON decode error for URL {url}: {json_err}")
        return None

def collect_item_ids(session):
    """
    Traverse all categories, alphas, and pages to collect item IDs per category.
    Returns a dictionary mapping category_id to a list of item_ids.
    """
    category_item_map = {}

    for category_id, category_name in tqdm(CATEGORY_MAPPING.items(), desc="Processing Categories"):
        logging.info(f"Processing Category {category_id}: {category_name}")
        print(f"Processing Category {category_id}: {category_name}")

        category_url = BASE_CATEGORY_URL.format(category_id)
        category_data = fetch_json(category_url, session)

        if not category_data:
            logging.error(f"Failed to retrieve data for category {category_id}")
            continue

        alpha_list = category_data.get("alpha", [])
        logging.debug(f"Alpha list for category {category_id}: {alpha_list}")

        # Initialize list for this category
        category_item_map[category_id] = []

        for alpha_entry in alpha_list:
            letter = alpha_entry.get("letter")
            items_count = alpha_entry.get("items", 0)

            if items_count == 0:
                logging.info(f"  Letter '{letter}' has no items.")
                print(f"  Letter '{letter}' has no items.")
                continue  # No items for this letter

            # Encode '#' as '%23' if necessary
            encoded_letter = "%23" if letter == "#" else quote(letter.lower())

            # Determine the number of items per page
            # Assuming 12 items per page based on previous logs
            items_per_page = 12
            total_pages = ceil(items_count / items_per_page)

            logging.info(f"  Letter '{letter}' has {items_count} items across {total_pages} pages.")
            print(f"  Letter '{letter}' has {items_count} items across {total_pages} pages.")

            for page in tqdm(range(1, total_pages + 1), desc=f"    Fetching pages for '{letter}'", leave=False):
                items_url = BASE_ITEMS_URL.format(category_id, encoded_letter, page)
                items_data = fetch_json(items_url, session)

                if not items_data:
                    logging.error(f"    Failed to fetch items for category {category_id}, letter '{letter}', page {page}")
                    continue

                items = items_data.get("items", [])
                for item in items:
                    item_id = item.get("id")
                    if item_id:
                        category_item_map[category_id].append(item_id)

                time.sleep(REQUEST_DELAY)  # Respectful delay

    return category_item_map

def save_to_json(category_item_map, filename="category_item_map.json"):
    """
    Save the category-item mapping to a JSON file.
    """
    try:
        with open(filename, 'w') as f:
            json.dump(category_item_map, f, indent=4)
        logging.info(f"Category-item mapping saved to {filename}")
        print(f"Category-item mapping saved to {filename}")
    except Exception as e:
        logging.error(f"Failed to save category-item mapping to {filename}: {e}")
        print(f"Failed to save category-item mapping to {filename}: {e}")

def update_database(session_db, category_item_map):
    """
    Perform a bulk update on the database to assign category_id to items based on the collected item IDs.
    """
    try:
        total_updates = 0
        for category_id, item_ids in category_item_map.items():
            if not item_ids:
                continue

            # PostgreSQL allows updating multiple rows using the ANY() operator
            update_stmt = text("""
                UPDATE items
                SET category_id = :category_id
                WHERE id = ANY(:item_ids)
            """)
            result = session_db.execute(update_stmt, {"category_id": category_id, "item_ids": item_ids})
            total_updates += result.rowcount
            logging.info(f"Updated {result.rowcount} items for category ID {category_id}")

        session_db.commit()
        logging.info(f"Bulk update completed. Total items updated: {total_updates}")
        print(f"Bulk update completed. Total items updated: {total_updates}")
    except Exception as e:
        session_db.rollback()
        logging.error(f"Failed to perform bulk update: {e}")
        print(f"Failed to perform bulk update: {e}")

# ============================
# Main Execution Function
# ============================

def main():
    """Main function to execute the item categorization."""
    try:
        logging.info("Starting item categorization process...")
        print("Starting item categorization process...\n")

        # Load database configuration
        config = load_db_config(DB_CONFIG_PATH)

        # Create database engine
        engine = create_db_engine(config)

        # Create a new database session
        Session = sessionmaker(bind=engine)
        session_db = Session()

        # Create a requests session
        session_http = create_session()

        # Collect all item IDs per category
        category_item_map = collect_item_ids(session_http)

        # Optionally save the mapping to a JSON file
        save_to_json(category_item_map, filename="category_item_map.json")

        # Perform bulk update in the database
        update_database(session_db, category_item_map)

    except KeyboardInterrupt:
        logging.warning("Script interrupted by user.")
        print("\nScript interrupted by user. Exiting gracefully.")
    except Exception as e:
        logging.exception(f"An unexpected error occurred: {e}")
        print(f"An unexpected error occurred: {e}")
    finally:
        # Close the HTTP session
        session_http.close()
        logging.info("HTTP session closed.")
        print("HTTP session closed.")

        # Close the database session
        session_db.close()
        logging.info("Database session closed.")
        print("Database session closed.")

        logging.info("Item categorization process completed.")
        print("Item categorization process completed.")

# ============================
# Entry Point
# ============================

if __name__ == "__main__":
    main()

